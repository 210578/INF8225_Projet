# -*- coding: utf-8 -*-
"""DDPG_Lunar_Launder_CartPole.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EzoSr0HY3ucv5klRAZ2dJgPg9M13pLY1
"""

"""
Classic cart-pole system implemented by Rich Sutton et al.
Copied from http://incompleteideas.net/sutton/book/code/pole.c
permalink: https://perma.cc/C9ZM-652R
"""

import math
import random

class CartPole():
    def __init__(self):
        self._cart_mass = 0.31  # (kg)
        self._pole_mass = 0.055  # (kg)
        self._pole_length = 0.4  # (m)

        self.x_threshold = 1.0
        self.theta_threshold = 12 * 2 * math.pi / 360

        self._state = []
        self._done = True

    def reset(self):
        self._step = 0
        self._cart_position = math.tanh(random.gauss(0.0, 0.01)) * 4.8  # (m)
        self._cart_velocity = random.uniform(-0.05, 0.05)  # (m/s)
        initial_pole_angle=random.uniform(-0.05, 0.05)
        self._pole_angle =  (initial_pole_angle + math.pi) % (2 * math.pi) - math.pi  # (rad)
        self._pole_angular_velocity = random.uniform(-0.05, 0.05)  # (rad/s)

        # (CartPole-v0 uses numpy.ndarray for state,
        #  but here returns Python array.)
        self._state = [self._cart_position, self._cart_velocity, self._pole_angle, self._pole_angular_velocity]
        self._done = False
        return self._state

    def step(self, action: float):
        """
        Args:
            action: float value between -1.0 and 1.0
        """
        if self._done:
            raise Exception("Cannot run step() before reset")

        self._step += 1

        # Add a small random noise
        # (The agent won't succeed by applying zero force each time.)
        force = 1.0 * (action + random.uniform(-0.02, 0.02))

        total_mass = self._cart_mass + self._pole_mass
        pole_half_length = self._pole_length / 2
        pole_mass_length = self._pole_mass * pole_half_length

        cosTheta = math.cos(self._pole_angle)
        sinTheta = math.sin(self._pole_angle)

        temp = (
            force + pole_mass_length * self._pole_angular_velocity ** 2 * sinTheta
        ) / total_mass
        angularAccel = (9.8 * sinTheta - cosTheta * temp) / (
            pole_half_length
            * (4.0 / 3.0 - (self._pole_mass * cosTheta ** 2) / total_mass)
        )
        linearAccel = temp - (pole_mass_length * angularAccel * cosTheta) / total_mass

        self._cart_position = self._cart_position + 0.02 * self._cart_velocity
        self._cart_velocity = self._cart_velocity + 0.02 * linearAccel

        self._pole_angle = (
            self._pole_angle + 0.02 * self._pole_angular_velocity
        )
        self._pole_angle = (self._pole_angle + math.pi) % (2 * math.pi) - math.pi

        self._pole_angular_velocity = (
            self._pole_angular_velocity + 0.02 * angularAccel
        )

        # (CartPole-v0 uses numpy.ndarray for state,
        #  but here returns Python array.)
        self._state = [self._cart_position, self._cart_velocity, self._pole_angle, self._pole_angular_velocity]
        term = self._state[0] < -self.x_threshold or \
            self._state[0] > self.x_threshold or \
            self._state[2] < -self.theta_threshold or \
            self._state[2] > self.theta_threshold
        term = bool(term)
        trunc = (self._step == 500)
        trunc = bool(trunc)
        self._done = bool(term or trunc)
        return self._state, 1.0, term, trunc, {}

!pip install torch numpy matplotlib
!pip install torch numpy gymnasium matplotlib
import json
import random
import numpy as np
import torch
import torch.nn as nn
from torch.nn import functional as F

# env = CartPole()
!pip install swig
!pip uninstall -y gym gymnasium box2d-py Box2D
!pip install gymnasium[box2d]
import gymnasium as gym
env = gym.make("LunarLander-v3")

import pickle
import os
os.makedirs("saves", exist_ok=True)

# Pour diffÃ©rents environnements
state_dim = 8  # = 4 pour CartPole, 3 pour Pendulum
action_dim = 4      # = 1 pour CartPole, 1 pour Pendulum
max_action = 2.0  # = 1.0 pour CartPole, 2. pour Pendulum

# (1) Q-networks
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class QNet(nn.Module):
    def __init__(self, hidden_dim=64):
        super().__init__()

        self.hidden = nn.Linear(state_dim+action_dim, hidden_dim)
        self.output = nn.Linear(hidden_dim, 1)

    def forward(self, s, a):
        outs = torch.concat((s, a), dim=-1)
        outs = self.hidden(outs)
        outs = F.relu(outs)
        outs = self.output(outs)
        return outs

q_origin_model = QNet().to(device)  # Q_phi
q_target_model = QNet().to(device)  # Q_phi'
_ = q_target_model.requires_grad_(False)  # target model doen't need grad

# (2) Policy networks
class PolicyNet(nn.Module):
    def __init__(self, hidden_dim=64):
        super().__init__()

        self.hidden = nn.Linear(state_dim, hidden_dim)
        self.output = nn.Linear(hidden_dim, action_dim)

    def forward(self, s):
        outs = self.hidden(s)
        outs = F.relu(outs)
        outs = self.output(outs)
        outs = torch.tanh(outs)
        return outs

mu_origin_model = PolicyNet().to(device)  # mu_theta
mu_target_model = PolicyNet().to(device)  # mu_theta'
_ = mu_target_model.requires_grad_(False)  # target model doen't need grad

# (3) Function to optimize network parameters
gamma = 0.99
opt_q = torch.optim.AdamW(q_origin_model.parameters(), lr=0.0005)
opt_mu = torch.optim.AdamW(mu_origin_model.parameters(), lr=0.0005)

def optimize(states, actions, rewards, next_states, dones):
    # Convert to tensor
    states = torch.tensor(states, dtype=torch.float).to(device)
    actions = torch.tensor(actions, dtype=torch.float).to(device)
    # actions = actions.unsqueeze(dim=1)
    rewards = torch.tensor(rewards, dtype=torch.float).to(device)
    rewards = rewards.unsqueeze(dim=1)
    next_states = torch.tensor(next_states, dtype=torch.float).to(device)
    dones = torch.tensor(dones, dtype=torch.float).to(device)
    dones = dones.unsqueeze(dim=1)

    # Optimize critic loss
    opt_q.zero_grad()
    q_org = q_origin_model(states, actions)
    with torch.no_grad():
        mu_tgt_next = mu_target_model(next_states)
        q_tgt_next = q_target_model(next_states, mu_tgt_next)
    q_tgt = rewards + gamma * (1.0 - dones) * q_tgt_next
    loss_q = F.mse_loss(
        q_org,
        q_tgt,
        reduction="none")
    loss_q.sum().backward()
    opt_q.step()

    # Optimize actor loss
    opt_mu.zero_grad()
    mu_org = mu_origin_model(states)
    for p in q_origin_model.parameters():
        p.requires_grad = False # disable grad in q_origin_model before computation
    q_tgt_max = q_origin_model(states, mu_org)
    (-q_tgt_max).sum().backward()
    opt_mu.step()
    for p in q_origin_model.parameters():
        p.requires_grad = True # enable grad again

# (4) Function to update target parameters
tau = 0.002

def update_target():
    for var, var_target in zip(q_origin_model.parameters(), q_target_model.parameters()):
        var_target.data = tau * var.data + (1.0 - tau) * var_target.data
    for var, var_target in zip(mu_origin_model.parameters(), mu_target_model.parameters()):
        var_target.data = tau * var.data + (1.0 - tau) * var_target.data

# (5) Replay buffer
class replayBuffer:
    def __init__(self, buffer_size: int):
        self.buffer_size = buffer_size
        self.buffer = []

    def add(self, item):
        if len(self.buffer) == self.buffer_size:
            self.buffer.pop(0)
        self.buffer.append(item)

    def sample(self, batch_size):
        items = random.sample(self.buffer, batch_size)
        states   = [i[0] for i in items]
        actions  = [i[1] for i in items]
        rewards  = [i[2] for i in items]
        n_states = [i[3] for i in items]
        dones    = [i[4] for i in items]
        return states, actions, rewards, n_states, dones

    def length(self):
        return len(self.buffer)

buffer = replayBuffer(buffer_size=20000)

# (6) Noise for exploration
"""
Ornstein-Uhlenbeck noise implemented by OpenAI
Copied from https://github.com/openai/baselines/blob/master/baselines/ddpg/noise.py
"""
class OrnsteinUhlenbeckActionNoise:
    def __init__(self, mu, sigma, theta=.15, dt=1e-2, x0=None):
        self.theta = theta
        self.mu = mu
        self.sigma = sigma
        self.dt = dt
        self.x0 = x0
        self.reset()

    def __call__(self):
        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)
        self.x_prev = x
        return x

    def reset(self):
        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)

ou_action_noise = OrnsteinUhlenbeckActionNoise(mu=np.zeros(1), sigma=np.ones(1) * 0.05)

# pick up action with Ornstein-Uhlenbeck noise
def pick_sample(s):
    with torch.no_grad():
        s = np.array(s)
        s_batch = np.expand_dims(s, axis=0)
        s_batch = torch.tensor(s_batch, dtype=torch.float).to(device)
        action_probs = mu_origin_model(s_batch)
        action_probs = F.softmax(action_probs, dim=-1)
        action = torch.multinomial(action_probs, 1).item()
        return action

# (7) Now let's put it all together !
batch_size = 250
import json

def action_to_one_hot(action):
    one_hot = np.zeros(action_dim)
    one_hot[action] = 1
    return one_hot

reward_records = []
for i in range(10000):
    # Run episode till done
    s, _ = env.reset()
    done = False
    cum_reward = 0
    while not done:
        a = pick_sample(s)
        s_next, r, term, trunc, _ = env.step(a)
        done = term or trunc
        buffer.add([s, action_to_one_hot(a), r, s_next, float(term)])  # (see above note for truncation)
        cum_reward += r

        # Train (optimize parameters)
        if buffer.length() >= batch_size:
            states, actions, rewards, n_states, dones = buffer.sample(batch_size)
            optimize(states, actions, rewards, n_states, dones)
            update_target()
        s = s_next

    # Output total rewards in episode (max 500)
    print("Run episode{} with rewards {}".format(i, cum_reward), end="\r")
    reward_records.append(cum_reward)

    with open("saves/reward_records.json", "w") as f:
        json.dump(reward_records, f)

    # stop if reward mean > 475.0
    if np.average(reward_records[-50:]) > 475.0:
        break

print("\nDone")

import matplotlib.pyplot as plt
# Generate recent 50 interval average
average_reward = []
for idx in range(len(reward_records)):
    avg_list = np.empty(shape=(1,), dtype=int)
    if idx < 50:
        avg_list = reward_records[:idx+1]
    else:
        avg_list = reward_records[idx-49:idx+1]
    average_reward.append(np.average(avg_list))
plt.plot(reward_records)
plt.plot(average_reward)